{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.filters import threshold_otsu, apply_hysteresis_threshold\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, disk\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from getpass import getpass\n",
    "import subprocess\n",
    "import omero_toolbox as omero\n",
    "\n",
    "from ilastik import app\n",
    "from ilastik.applets.dataSelection.opDataSelection import PreloadedArrayDatasetInfo  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setup some logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level='INFO')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may limit the number of threads and the RAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ[\"LAZYFLOW_THREADS\"] = \"8\"\n",
    "os.environ[\"LAZYFLOW_TOTAL_RAM_MB\"] = \"16000\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HOST = 'omero.mri.cnrs.fr'\n",
    "PORT = 4064\n",
    "TEMP_DIR = '~/DATA'\n",
    "MODELS_PATH = '../models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input some variables needed for the analysis\n",
    "\n",
    "Number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_count = int(input('Number of channels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The channel names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ch_names = []\n",
    "for c in range(ch_count):\n",
    "    ch_names.append(input(f'Channel name ch {c}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation thresholds for each channel and for the background. Must be a value between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "segmentation_thr = []\n",
    "for c in range(ch_count):\n",
    "    segmentation_thr.append(int(input(f'Threshold ch {c}')))\n",
    "segmentation_thr.append(int(input('Threshold background')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation upper and lower correction factors for each channel and for the background.\n",
    "Must be a value between 0.0 and 1.0 and the upper corrections must be equal or higher than the lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "upper_correction_factors = []\n",
    "lower_correction_factors = []\n",
    "\n",
    "for c in range(ch_count):\n",
    "    upper_correction_factors.append(float(input(f'Upper correction ch {c}')))\n",
    "    lower_correction_factors.append(float(input(f'Lower correction ch {c}')))\n",
    "\n",
    "upper_correction_factors.append(float(input('Threshold background')))\n",
    "lower_correction_factors.append(float(input('Threshold background')))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classifier model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROJECT_PATH = f'{MODELS_PATH}/{input(\"Model name\")}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to assign the correspondences between the channels in the classifier model and in the image and the background.\n",
    "This should work in case the input images order follow the order in the classifier being the last, additional, channel in the classifier the background.\n",
    "\n",
    "Modify if necessary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (input_image_channel, probabilities_ch)\n",
    "object_ch_match = [(c, c) for c in range(ch_count)]\n",
    "# (input_image_channel, background_probability_channel)\n",
    "ch_bg_match = [(c, ch_count) for c in range(ch_count)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining some functions we are going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_ilastik(input_data, model_path):\n",
    "    args = app.parse_args([])\n",
    "    args.headless = True\n",
    "    args.project = model_path\n",
    "\n",
    "    shell = app.main(args)\n",
    "\n",
    "    data = dict([(\n",
    "                  \"Raw Data\",\n",
    "                  [PreloadedArrayDatasetInfo(preloaded_array=input_data)],\n",
    "                  )])\n",
    "    predictions = shell.workflow.batchProcessingApplet.run_export(data,\n",
    "                                                                  export_to_array=True)  # noqa\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def segment_channel(channel, threshold=None, min_distance=2, remove_border=False, low_corr_factor=1, high_corr_factor=1):\n",
    "    \"\"\"Segment a channel (3D numpy array)\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        threshold = threshold_otsu(channel)\n",
    "\n",
    "    thresholded = apply_hysteresis_threshold(channel,\n",
    "                                             low=threshold * low_corr_factor,\n",
    "                                             high=threshold * high_corr_factor\n",
    "                                             )\n",
    "\n",
    "    thresholded = closing(thresholded, disk(min_distance))\n",
    "    if remove_border:\n",
    "        thresholded = clear_border(thresholded)\n",
    "    return label(thresholded)\n",
    "\n",
    "\n",
    "def segment_image(image,\n",
    "                  thresholds=None,\n",
    "                  low_corr_factors=None,\n",
    "                  high_corr_factors=None):\n",
    "    \"\"\"Segment an image and return a labels object.\n",
    "    Image must be provided as cyx numpy array\n",
    "    \"\"\"\n",
    "    if len(image.shape) < 3:\n",
    "        image = np.expand_dims(image, 0)\n",
    "\n",
    "    if low_corr_factors is None:\n",
    "        low_corr_factors = [.95] * image.shape[0]\n",
    "    if high_corr_factors is None:\n",
    "        high_corr_factors = [1.05] * image.shape[0]\n",
    "\n",
    "    if len(high_corr_factors) != image.shape[0] or len(low_corr_factors) != image.shape[0]:\n",
    "        raise Exception('The number of correction factors does not match the number of channels.')\n",
    "\n",
    "    # We create an empty array to store the output\n",
    "    labels_image = np.zeros(image.shape, dtype=np.uint16)\n",
    "    for c in range(image.shape[0]):\n",
    "        threshold = thresholds[c] if thresholds is not None else None\n",
    "        labels_image[c, ...] = segment_channel(image[c, ...],\n",
    "                                               threshold=threshold,\n",
    "                                               low_corr_factor=low_corr_factors[c],\n",
    "                                               high_corr_factor=high_corr_factors[c])\n",
    "    return labels_image\n",
    "\n",
    "\n",
    "def compute_channel_spots_properties(channel, label_channel):\n",
    "    \"\"\"Analyzes and extracts the properties of a single channel\"\"\"\n",
    "\n",
    "    ch_properties = []\n",
    "    logger.info(f'label_channel dims: {label_channel.shape}')\n",
    "    logger.info(f'channel dims: {channel.shape}')\n",
    "    regions = regionprops(label_channel, channel)\n",
    "\n",
    "    for region in regions:\n",
    "        ch_properties.append({'label': region.label,\n",
    "                              'area': region.area,\n",
    "                              'centroid_x': region.centroid[1],\n",
    "                              'centroid_y': region.centroid[0],\n",
    "                              'eccentricity': region.eccentricity,\n",
    "                              'perimeter': region.perimeter,\n",
    "                              'max_intensity': region.max_intensity,\n",
    "                              'mean_intensity': region.mean_intensity,\n",
    "                              'min_intensity': region.min_intensity,\n",
    "                              'integrated_intensity': region.mean_intensity * region.area\n",
    "                              })\n",
    "\n",
    "    return ch_properties\n",
    "\n",
    "\n",
    "def compute_spots_properties(image, labels):\n",
    "    \"\"\"Computes a number of properties for the PSF-like spots found on an image provided they are segmented\"\"\"\n",
    "    # TODO: Verify dimensions of image and labels are the same\n",
    "    properties = []\n",
    "\n",
    "    for c in range(image.shape[0]):  # TODO: Deal with Time here\n",
    "        pr = compute_channel_spots_properties(channel=image[c, :, :],\n",
    "                                              label_channel=labels[c, :, :],\n",
    "                                              )\n",
    "        properties.append(pr)\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    # Open the connection to OMERO\n",
    "    conn = omero.open_connection(username=input(\"Username: \"),\n",
    "                                 password=getpass(\"OMERO Password: \", None),\n",
    "                                 host=str(input('server (omero.mri.cnrs.fr): ') or HOST),\n",
    "                                 port=int(input('port (4064): ') or PORT),\n",
    "                                 group=input(\"Group: \"))\n",
    "    conn.c.enableKeepAlive(60)\n",
    "\n",
    "    # get tagged images in dataset\n",
    "    dataset_id = int(input('Dataset ID: '))\n",
    "    dataset = omero.get_dataset(conn, dataset_id)\n",
    "    project = dataset.getParent()\n",
    "\n",
    "    new_dataset_name = f'{dataset.getName()}_ilastik_output'\n",
    "    new_dataset_description = f'Source Dataset ID: {dataset.getId()}'\n",
    "    new_dataset = omero.create_dataset(conn,\n",
    "                                       name=new_dataset_name,\n",
    "                                       description=new_dataset_description,\n",
    "                                       parent_project=project)\n",
    "\n",
    "    images = dataset.listChildren()\n",
    "\n",
    "    images_names_ids = {i.getName(): i.getId() for i in images}\n",
    "    image_root_names = list(set([n[:-4] for n in images_names_ids.keys()]))\n",
    "\n",
    "    table_col_names = ['image_id',\n",
    "                       'image_name',\n",
    "                       'mouse_nr',\n",
    "                       'replica_nr',\n",
    "                       'genotype',\n",
    "                       'treatment',\n",
    "                       'roi_area']\n",
    "\n",
    "    for ch_name in ch_names:\n",
    "        table_col_names.extend([f'roi_intensity_{ch_name}',\n",
    "                                f'object_count_{ch_name}',\n",
    "                                f'mean_area_{ch_name}',\n",
    "                                f'median_area_{ch_name}',\n",
    "                                f'sum_area_{ch_name}',\n",
    "                                f'sum_intensity_{ch_name}',\n",
    "                                f'mean_intensity_{ch_name}',\n",
    "                                f'sum_area_bg_{ch_name}',\n",
    "                                f'sum_intensity_bg_{ch_name}',\n",
    "                                f'mean_intensity_bg_{ch_name}'\n",
    "                                ])\n",
    "    table_col_values = [[] for _ in range(len(table_col_names))]\n",
    "\n",
    "    for counter, image_root_name in enumerate(image_root_names):\n",
    "        logger.info(f'Analyzing image {image_root_name}')\n",
    "\n",
    "        mip_image = conn.getObject('Image', images_names_ids[f'{image_root_name}_MIP'])\n",
    "        mip_data = omero.get_intensities(mip_image)\n",
    "        aip_image = conn.getObject('Image', images_names_ids[f'{image_root_name}_AIP'])\n",
    "        aip_data = omero.get_intensities(aip_image)\n",
    "\n",
    "        # Filling data table\n",
    "        name_md = image_root_name.strip()\n",
    "        name_md = name_md.replace(' ', '_').split('_')\n",
    "\n",
    "        table_col_values[0].append(aip_image)  # 'image_id'\n",
    "        table_col_values[1].append(image_root_name)  # 'image_name'\n",
    "        table_col_values[2].append(name_md[0])  # 'mouse_nr'\n",
    "        table_col_values[3].append(name_md[1])  # 'replica_nr'\n",
    "        table_col_values[4].append(name_md[2])  # 'genotype'\n",
    "        table_col_values[5].append(name_md[3])  # 'treatment'\n",
    "\n",
    "        # Some basic measurements\n",
    "        roi_area = np.count_nonzero(aip_data[0, 0, 0, ...])\n",
    "        table_col_values[6].append(roi_area)  # 'roi_area'\n",
    "\n",
    "        # We were downloading the images without the z dimension so we have to remove it here\n",
    "        # mip_data = mip_data.squeeze(axis=0)\n",
    "\n",
    "        temp_file = f'{TEMP_DIR}/{mip_image.getName()}.npy'\n",
    "        np.save(temp_file, mip_data)\n",
    "\n",
    "        run_ilastik(mip_data, PROJECT_PATH)\n",
    "\n",
    "        output_file = f'{TEMP_DIR}/{mip_image.getName()}_Probabilities.npy'\n",
    "        prob_data = np.load(output_file)\n",
    "\n",
    "        # Save the output back to OMERO\n",
    "        omero.create_image_from_numpy_array(connection=conn,\n",
    "                                            data=prob_data,\n",
    "                                            image_name=f'{mip_image.getName()}_PROB',\n",
    "                                            image_description=f'Source Image ID:{mip_image.getId()}',\n",
    "                                            dataset=new_dataset,\n",
    "                                            channel_labels=ch_names + ['background'],\n",
    "                                            force_whole_planes=False\n",
    "                                            )\n",
    "\n",
    "        prob_data = prob_data.squeeze()\n",
    "        aip_data = aip_data.squeeze()\n",
    "\n",
    "        for object_ch, bg_ch in zip(object_ch_match, ch_bg_match):\n",
    "            # Keep connection alive\n",
    "            conn.keepAlive()\n",
    "            # Calculate object properties on the objects\n",
    "            object_labels = segment_channel(channel=prob_data[object_ch[1]], threshold=segmentation_thr[object_ch[1]])\n",
    "            object_properties = compute_channel_spots_properties(channel=aip_data[object_ch[0]], label_channel=object_labels)\n",
    "            object_df = pd.DataFrame(object_properties)\n",
    "\n",
    "            # Calculate properties of the background\n",
    "            bg_labels = segment_channel(channel=prob_data[bg_ch[1]], threshold=segmentation_thr[bg_ch[1]])\n",
    "            bg_properties = compute_channel_spots_properties(channel=aip_data[bg_ch[0]], label_channel=bg_labels)\n",
    "            bg_df = pd.DataFrame(bg_properties)\n",
    "\n",
    "            # Save dataframes as csv attachments to the images\n",
    "            object_df.to_csv(f'{TEMP_DIR}/ch{object_ch[0]}_object_df.csv')\n",
    "            object_csv_ann = omero.create_annotation_file_local(\n",
    "                connection=conn,\n",
    "                file_path=f'{TEMP_DIR}/ch{object_ch[0]}_object_df.csv',\n",
    "                description=f'Data corresponding to the objects on channel {object_ch[0]}')\n",
    "            omero.link_annotation(aip_image, object_csv_ann)\n",
    "\n",
    "            bg_df.to_csv(f'{TEMP_DIR}/ch{bg_ch[0]}_bg_df.csv')\n",
    "            bg_csv_ann = omero.create_annotation_file_local(\n",
    "                connection=conn,\n",
    "                file_path=f'{TEMP_DIR}/ch{bg_ch[0]}_bg_df.csv',\n",
    "                description=f'Data corresponding to the background on channel {bg_ch[0]}')\n",
    "            omero.link_annotation(aip_image, bg_csv_ann)\n",
    "\n",
    "            if len(object_df) > 0:\n",
    "                table_col_values[table_col_names.index(f'roi_intensity_{ch_names[object_ch[0]]}')].append(np.sum(aip_data[object_ch[0]]).item())\n",
    "                table_col_values[table_col_names.index(f'object_count_{ch_names[object_ch[0]]}')].append(len(object_df))\n",
    "\n",
    "                table_col_values[table_col_names.index(f'mean_area_{ch_names[object_ch[0]]}')].append(object_df['area'].mean().item())\n",
    "                table_col_values[table_col_names.index(f'median_area_{ch_names[object_ch[0]]}')].append(object_df['area'].median().item())\n",
    "                table_col_values[table_col_names.index(f'sum_area_{ch_names[object_ch[0]]}')].append(object_df['area'].sum().item())\n",
    "                table_col_values[table_col_names.index(f'sum_intensity_{ch_names[object_ch[0]]}')].append(object_df['integrated_intensity'].sum().item())\n",
    "                table_col_values[table_col_names.index(f'mean_intensity_{ch_names[object_ch[0]]}')].append(object_df['integrated_intensity'].sum().item() /\n",
    "                                                                                                           object_df['area'].sum().item())\n",
    "                table_col_values[table_col_names.index(f'sum_area_bg_{ch_names[object_ch[0]]}')].append(bg_df['area'].sum().item())\n",
    "                table_col_values[table_col_names.index(f'sum_intensity_bg_{ch_names[object_ch[0]]}')].append(bg_df['integrated_intensity'].sum().item())\n",
    "                table_col_values[table_col_names.index(f'mean_intensity_bg_{ch_names[object_ch[0]]}')].append(bg_df['integrated_intensity'].sum().item() /\n",
    "                                                                                                              bg_df['area'].sum().item())\n",
    "            else:\n",
    "                logger.warning(f'No objects were detected for image {image_root_name}')\n",
    "\n",
    "                table_col_values[table_col_names.index(f'roi_intensity_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'object_count_{ch_names[object_ch[0]]}')].append(0)\n",
    "\n",
    "                table_col_values[table_col_names.index(f'mean_area_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'median_area_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'sum_area_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'sum_intensity_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'mean_intensity_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'sum_area_bg_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'sum_intensity_bg_{ch_names[object_ch[0]]}')].append(0)\n",
    "                table_col_values[table_col_names.index(f'mean_intensity_bg_{ch_names[object_ch[0]]}')].append(0)\n",
    "\n",
    "        logger.info(f'Processed image {counter}')\n",
    "\n",
    "    table = omero.create_annotation_table(connection=conn,\n",
    "                                          table_name='Aggregated_measurements',\n",
    "                                          column_names=table_col_names,\n",
    "                                          column_descriptions=table_col_names,\n",
    "                                          values=table_col_values,\n",
    "                                          )\n",
    "    omero.link_annotation(dataset, table)\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "    logger.info('Done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}